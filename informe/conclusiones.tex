\section{Conclusiones generales}

Como primer punto, debemos destacar que todas las conclusiones incluidas aquí derivan de experimentaciones con instancias particulares, y si bien enfocamos nuestros esfuerzos en evitar sesgar los experimentos, debemos advertir que para casos patológicos, estas conclusiones podrían no aplicar. Además, queremos marcar que el hecho particular de haber utilizado ciertas distribuciones de probabilidad para generar instancias de test también lleva a que no consideremos casos de test posibles, de hecho, trabajo a futuro interesante podría involucrar investigar instancias generadas con otras distribuciones y observar una vez más los criterios de convergencia. Estimamos que muy probablemente sea posible encontrar clases de instancias que converjan con valores de granularidad fijos.

A raíz de la comprensión de los algoritmos planteados, detallamos una breve descripción de mejoras posibles a la implementación, que no fueron incluidas en el código:

\begin{enumerate}
\item Si observamos los algoritmos de eliminación de sanguijuelas, encontramos una posible optimización. Para estos casos, las sanguijuelas deben ser guardadas y eliminadas una a una. Pero podría ocurrir el caso de que existan dos sanguijuelas, llamémoslas $x$ y $z$, con $x$ de radio y temperatura menor a $z$, en que $x$ se encuentre posicionada de tal manera que todos los puntos de la discretizaci\'on afectados por $x$ también se encuentren afectados por $z$. En este caso, elimina $X$ y recalcular el modelo me daría el modelo original, por lo que al evitar que se compute dicho caso, no perdemos información, y mas importante, ahorramos tiempo de computo.

\item Además, no consideramos la posibilidad de utilizar Sherman Morrison para resolver casos donde las sanguijuelas afecten a más de una fila. Si bien esto es posible (y de hecho tiene un nombre, la Identidad de Woodbury), ya que tenemos que $(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}$ ~\cite[p.~330]{woodbury}, y podríamos intentar buscar las matrices $U$, $C$, y $V$ que nos permitan computar perturbaciones de mayor rango. Suponemos que este proceso no debe ser posible para hacer actualizaciones de rango demasiado grande, porque eso significaría que podríamos invertir matrices en $O(n^2)$, teniendo sólo la inversa de $A$, la de $C$, y modificando $U$ y $V$ para que la suma del producto nos termine dando lo que buscamos.
\end{enumerate}

Encontramos que para modelar el sistema sin modificarlo, eliminación gaussiana es la opción óptima por su menor costo temporal y espacial, así que resulta en la solución óptima al problema del modelado de temperaturas. Sin embargo, esta solución es poco amigable a la reutilización de datos característica del problema, por lo que la factorización LU y su posibilidad de amortizar costos terminó siendo una parte importante del trabajo.

Por otro lado, cuando debemos decidir si podemos o no salvar la nave, podemos optimizar el algoritmo para casos de sanguijuelas unitarias con Sherman y Morrison, pues el costo temporal ahorrado es significativo. En el momento de decidir si optimizar o no de esta forma, tener información particular sobre la instancia es crucial y, más aun, el costo de contar la cantidad de sanguijuelas unitarias es lo suficientemente bajo como para poder ser amortizado en los casos en los que Sherman Morrison es aplicable.

En general, observamos que discretizar la instancia en matrices de $100 \times 100$ mts tiene un muy buen resultado a nivel estimativo, y no resulta excesivamente caro de computar cuando nos restringimos a valores de granularidad razonables, como $h = 0.5$. Claramente, la decisión final depende del uso especifico que se le quiera dar a la instancia. No necesitamos la misma calidad de modelado para, por ejemplo, contar absolutamente todas las sanguijuelas en mi parabrisas, que para decidir en nanosegundos si debo disparar o no, porque mi parabrisas se esta por romper. Además, es muy importante tener en cuenta que en general las distintas instancias que encontramos solían converger a una cierta temperatura a partir de ciertos puntos de $h$, por lo que si se pudiera modelar en una situación concreta cuál sería la distribución sobre los parámetros de las sanguijuelas, sería posible encontrar un $h$ a partir del cual la mayoría de los casos convergerían a solución de buena calidad.

Pensamos que sería interesante también analizar qué sucede con la complejidad temporal en función de la cantidad de sanguijuelas de una instancia, y su relación con el factor de granularidad $h$: ver cuál es el trade-off de tener muchas sanguijuelas. Suponemos probable que la temperatura se mantendría estable a pesar de aumentar el $h$, ya que una alta densidad de sanguijuelas en la discretización jamás terminaría de hacer desaparecer a todas y derivaría en que podríamos tomar valores de $h$ mayores a los que utilizamos, permitiendo soluciones de mayor velocidad al problema, donde la perdida de precision se vea amortizada por la cantidad de sanguijuelas.

El algoritmo de factorización LU es menos eficiente que Eliminación Gaussiana para valores muy chicos de h, pero para la reutilización de datos nos conviene usar LU ya que no tenemos que volver a factorizar la matriz principal, sino solo que tenemos que resolver el sistema en $O(n^2)$. 

Para valores de h menores a 0.5 crece exponencialmente el costo computacional para los algoritmos de Eliminación Gaussiana y Factorización LU. 
